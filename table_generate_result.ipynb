{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15128.,  6969.,  9000., 11717.,  7985.,  6481.,  7815., 13763.,\n",
       "       11011.,  6301., 11930.,  8227.,  9688.,  9730.,  6083.,  5855.,\n",
       "        7523., 11941., 10667.,  6135.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistic n_tweets_in_topic\n",
    "import numpy as np\n",
    "\n",
    "topic_result_path = \"./output/topic_result_2023-04-22-13-40-46_1iter.txt\"\n",
    "reader = open(topic_result_path, \"r\")\n",
    "\n",
    "n_topic = 20\n",
    "n_tweets_in_topic = np.zeros((n_topic))\n",
    "for line in reader.readlines():\n",
    "    split_line = line.split(\" \")\n",
    "    for i in range(len(split_line)):\n",
    "        if split_line[i] == \"(topic:\":\n",
    "            n_tweets_in_topic[int(split_line[i + 1].split(')')[0])] += 1\n",
    "            \n",
    "n_tweets_in_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                          text_clean\n",
       " 0  ai take job within five year expert warn exper...\n",
       " 1  although chatgpt rapidli gain popular also bec...\n",
       " 2  amplifi human potenti school educ board readi ...\n",
       " 3  analyst eran shimoni omer tsarfati detail crea...\n",
       " 4  artificialintellig ai take job within five yea...,\n",
       " ['ai take job within five year expert warn expert explain bot like chatgpt domin labor market via  (topic: 7)',\n",
       "  'although chatgpt rapidli gain popular also becom issu concern openaichatgpt ai nlp  (topic: 17)',\n",
       "  'amplifi human potenti school educ board readi aiin aiineduc chatgpt educ  (topic: 3)',\n",
       "  'analyst eran shimoni omer tsarfati detail creat polymorph malwar use chatgpt plan releas work learn purpos  (topic: 13)',\n",
       "  'artificialintellig ai take job within five year expert explain bot like chatgpt domin labor market uselesseat take vax  (topic: 7)'],\n",
       " (183949, 1),\n",
       " 183949)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "\"\"\"\n",
    "tweets - '.csv' organization of the original corpus\n",
    "tweets_btm - Topic model text file after obtaining the topic classification results, organized as \"document (Topic: 8)\"\n",
    "\"\"\"\n",
    "after_preprocess_dataset_path = \"./data/after_preprocess_dataset_clean_english_only_new.csv\"\n",
    "topic_result_path = \"./output/topic_result_2023-04-22-13-40-46_1iter.txt\"\n",
    "\n",
    "tweets = pd.read_csv(after_preprocess_dataset_path)\n",
    "tweets_btm = open(topic_result_path).read().splitlines()\n",
    "tweets.head(5), tweets_btm[:5], tweets.shape, len(tweets_btm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai take job within five year expert warn exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>although chatgpt rapidli gain popular also bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amplifi human potenti school educ board readi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analyst eran shimoni omer tsarfati detail crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialintellig ai take job within five yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ai take job within five year expert warn exper...\n",
       "1  although chatgpt rapidli gain popular also bec...\n",
       "2  amplifi human potenti school educ board readi ...\n",
       "3  analyst eran shimoni omer tsarfati detail crea...\n",
       "4  artificialintellig ai take job within five yea..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.rename(columns={\"text_clean\": \"text\"}, inplace=True) # Change the table header to 'text'\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the BTM model file\n",
    "model_path = \"./models/btm_model_2023-04-22-13-40-46_1iter.pkl\"\n",
    "f = open(model_path,'rb')\n",
    "biterm_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt,work,ai,use,job,write,openai,need,ask,human,tool,gpt,gener,new,help,like,industri,concern,creat,skill,\n",
      "chatgpt,ai,work,like,use,gpt,look,job,team,ask,time,tool,educ,openai,industri,research,write,gener,skill,task,\n",
      "chatgpt,ai,work,use,like,job,help,task,educ,make,need,assist,gener,model,new,content,way,openai,time,tool,\n",
      "chatgpt,ai,work,educ,use,job,write,think,intellig,gener,artifici,tool,thing,school,new,student,human,technolog,chatbot,need,\n",
      "chatgpt,ai,work,job,use,write,like,gpt,make,gener,tool,industri,train,peopl,help,good,new,data,way,human,\n",
      "chatgpt,ai,job,work,use,like,make,dont,task,ask,think,data,futur,industri,human,peopl,train,better,languag,skill,\n",
      "chatgpt,ai,job,work,use,new,like,task,learn,ask,data,help,futur,make,question,tool,technolog,gpt,think,busi,\n",
      "chatgpt,ai,use,work,like,job,creat,googl,gener,make,task,new,train,write,educ,help,answer,model,time,prompt,\n",
      "chatgpt,ai,work,use,time,job,like,think,human,gpt,team,tech,ask,peopl,make,technolog,educ,tri,code,world,\n",
      "chatgpt,ai,work,use,languag,model,train,write,data,task,job,creat,new,like,skill,educ,human,question,tool,make,\n",
      "chatgpt,ai,work,job,use,like,openai,gener,way,time,make,educ,learn,human,skill,train,technolog,say,student,need,\n",
      "chatgpt,ai,work,job,new,use,like,openai,peopl,gener,good,googl,ask,microsoft,think,skill,inform,make,human,time,\n",
      "chatgpt,ai,work,use,educ,googl,like,job,creat,ask,write,learn,task,code,skill,human,tool,tri,futur,new,\n",
      "chatgpt,ai,use,job,work,like,make,tool,task,help,team,gener,industri,technolog,openai,dont,write,googl,better,learn,\n",
      "chatgpt,ai,use,work,job,team,gener,tool,creat,train,come,new,data,futur,learn,great,write,languag,educ,opportun,\n",
      "chatgpt,ai,use,work,job,write,gpt,know,like,gener,new,student,world,ask,tool,read,team,market,time,day,\n",
      "chatgpt,ai,work,use,job,write,make,like,tool,help,task,time,team,ask,new,creat,learn,product,openai,text,\n",
      "chatgpt,ai,work,use,job,write,educ,like,gener,make,tool,think,way,know,human,openai,creat,industri,replac,learn,\n",
      "chatgpt,ai,use,work,gener,gpt,task,job,like,openai,tool,model,human,write,time,educ,power,content,make,text,\n",
      "chatgpt,ai,work,job,like,time,task,good,alreadi,think,gpt,ask,learn,use,replac,help,educ,peopl,improv,human,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chatgpt,work,ai,use,job,write,openai,need,ask,human,tool,gpt,gener,new,help,like,industri,concern,creat,skill,',\n",
       " 'chatgpt,ai,work,like,use,gpt,look,job,team,ask,time,tool,educ,openai,industri,research,write,gener,skill,task,',\n",
       " 'chatgpt,ai,work,use,like,job,help,task,educ,make,need,assist,gener,model,new,content,way,openai,time,tool,',\n",
       " 'chatgpt,ai,work,educ,use,job,write,think,intellig,gener,artifici,tool,thing,school,new,student,human,technolog,chatbot,need,',\n",
       " 'chatgpt,ai,work,job,use,write,like,gpt,make,gener,tool,industri,train,peopl,help,good,new,data,way,human,',\n",
       " 'chatgpt,ai,job,work,use,like,make,dont,task,ask,think,data,futur,industri,human,peopl,train,better,languag,skill,',\n",
       " 'chatgpt,ai,job,work,use,new,like,task,learn,ask,data,help,futur,make,question,tool,technolog,gpt,think,busi,',\n",
       " 'chatgpt,ai,use,work,like,job,creat,googl,gener,make,task,new,train,write,educ,help,answer,model,time,prompt,',\n",
       " 'chatgpt,ai,work,use,time,job,like,think,human,gpt,team,tech,ask,peopl,make,technolog,educ,tri,code,world,',\n",
       " 'chatgpt,ai,work,use,languag,model,train,write,data,task,job,creat,new,like,skill,educ,human,question,tool,make,',\n",
       " 'chatgpt,ai,work,job,use,like,openai,gener,way,time,make,educ,learn,human,skill,train,technolog,say,student,need,',\n",
       " 'chatgpt,ai,work,job,new,use,like,openai,peopl,gener,good,googl,ask,microsoft,think,skill,inform,make,human,time,',\n",
       " 'chatgpt,ai,work,use,educ,googl,like,job,creat,ask,write,learn,task,code,skill,human,tool,tri,futur,new,',\n",
       " 'chatgpt,ai,use,job,work,like,make,tool,task,help,team,gener,industri,technolog,openai,dont,write,googl,better,learn,',\n",
       " 'chatgpt,ai,use,work,job,team,gener,tool,creat,train,come,new,data,futur,learn,great,write,languag,educ,opportun,',\n",
       " 'chatgpt,ai,use,work,job,write,gpt,know,like,gener,new,student,world,ask,tool,read,team,market,time,day,',\n",
       " 'chatgpt,ai,work,use,job,write,make,like,tool,help,task,time,team,ask,new,creat,learn,product,openai,text,',\n",
       " 'chatgpt,ai,work,use,job,write,educ,like,gener,make,tool,think,way,know,human,openai,creat,industri,replac,learn,',\n",
       " 'chatgpt,ai,use,work,gener,gpt,task,job,like,openai,tool,model,human,write,time,educ,power,content,make,text,',\n",
       " 'chatgpt,ai,work,job,like,time,task,good,alreadi,think,gpt,ask,learn,use,replac,help,educ,peopl,improv,human,']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "\n",
    "# vectorize texts\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "tweets_list = [i for item in tweets.values for i in item]\n",
    "X = vec.fit_transform(tweets_list).toarray()\n",
    "\n",
    "# vocab - Get all words\n",
    "vocab = np.array([t for t, i in sorted(vec.vocabulary_.items(),\n",
    "                                     key=itemgetter(1))])\n",
    "\n",
    "# The top probability list of the most likely words for each topic\n",
    "topic_top_prob = biterm_model.phi_wz # phi_wz: [word, topic] Word distribution probability on each topic\n",
    "\n",
    "# Find the topM words for each topic\n",
    "def generate_topic_top_word(topic_top_prob, V, M = 10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        topic_top_prob - The top probability list of the most likely words for each topic\n",
    "        V [List] - A list of all the words\n",
    "        M - Take the first M words\n",
    "    Returns:\n",
    "        topic_top_word Dict(List[Tuple()]) - The names of the top M words in probability\n",
    "    \"\"\"\n",
    "    topic_top_word = dict()\n",
    "    for z, P_wzi in enumerate(topic_top_prob.T): \n",
    "        \"\"\"\n",
    "            z - z-th topic\n",
    "            P_wzi - The probability distribution of all words on the z-th topic\n",
    "        \"\"\"\n",
    "        topic_top_word[z] = [] # Each topic consists of multiple tuples (word, prob)\n",
    "        V_z_prob = np.sort(P_wzi)[:-(M + 1):-1] # Sort the probability distribution\n",
    "        V_z = np.argsort(P_wzi)[:-(M + 1):-1] # Sort the probability distribution and find the index of the top words\n",
    "        W_z = V[V_z] # Find the name of the word at the top of the list\n",
    "        for prob, word in zip(V_z_prob, W_z): # Form the innermost tuple, meaning tuple(word, prob).\n",
    "            topic_top_word[z].append((word, prob))\n",
    "    return topic_top_word\n",
    "\n",
    "topic_all_words = []\n",
    "# top-M words for each topic\n",
    "topic_top_word = generate_topic_top_word(topic_top_prob, vocab, 20)\n",
    "for i in range(20):\n",
    "    print_str = \"\"\n",
    "    # print_str = f\"topic {i}\"\n",
    "    for j in range(len(topic_top_word[i])):\n",
    "        print_str += f\"{topic_top_word[i][j][0]},\"\n",
    "    topic_all_words.append(print_str)\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0/183949\n",
      "finished 1000/183949\n",
      "finished 2000/183949\n",
      "finished 3000/183949\n",
      "finished 4000/183949\n",
      "finished 5000/183949\n",
      "finished 6000/183949\n",
      "finished 7000/183949\n",
      "finished 8000/183949\n",
      "finished 9000/183949\n",
      "finished 10000/183949\n",
      "finished 11000/183949\n",
      "finished 12000/183949\n",
      "finished 13000/183949\n",
      "finished 14000/183949\n",
      "finished 15000/183949\n",
      "finished 16000/183949\n",
      "finished 17000/183949\n",
      "finished 18000/183949\n",
      "finished 19000/183949\n",
      "finished 20000/183949\n",
      "finished 21000/183949\n",
      "finished 22000/183949\n",
      "finished 23000/183949\n",
      "finished 24000/183949\n",
      "finished 25000/183949\n",
      "finished 26000/183949\n",
      "finished 27000/183949\n",
      "finished 28000/183949\n",
      "finished 29000/183949\n",
      "finished 30000/183949\n",
      "finished 31000/183949\n",
      "finished 32000/183949\n",
      "finished 33000/183949\n",
      "finished 34000/183949\n",
      "finished 35000/183949\n",
      "finished 36000/183949\n",
      "finished 37000/183949\n",
      "finished 38000/183949\n",
      "finished 39000/183949\n",
      "finished 40000/183949\n",
      "finished 41000/183949\n",
      "finished 42000/183949\n",
      "finished 43000/183949\n",
      "finished 44000/183949\n",
      "finished 45000/183949\n",
      "finished 46000/183949\n",
      "finished 47000/183949\n",
      "finished 48000/183949\n",
      "finished 49000/183949\n",
      "finished 50000/183949\n",
      "finished 51000/183949\n",
      "finished 52000/183949\n",
      "finished 53000/183949\n",
      "finished 54000/183949\n",
      "finished 55000/183949\n",
      "finished 56000/183949\n",
      "finished 57000/183949\n",
      "finished 58000/183949\n",
      "finished 59000/183949\n",
      "finished 60000/183949\n",
      "finished 61000/183949\n",
      "finished 62000/183949\n",
      "finished 63000/183949\n",
      "finished 64000/183949\n",
      "finished 65000/183949\n",
      "finished 66000/183949\n",
      "finished 67000/183949\n",
      "finished 68000/183949\n",
      "finished 69000/183949\n",
      "finished 70000/183949\n",
      "finished 71000/183949\n",
      "finished 72000/183949\n",
      "finished 73000/183949\n",
      "finished 74000/183949\n",
      "finished 75000/183949\n",
      "finished 76000/183949\n",
      "finished 77000/183949\n",
      "finished 78000/183949\n",
      "finished 79000/183949\n",
      "finished 80000/183949\n",
      "finished 81000/183949\n",
      "finished 82000/183949\n",
      "finished 83000/183949\n",
      "finished 84000/183949\n",
      "finished 85000/183949\n",
      "finished 86000/183949\n",
      "finished 87000/183949\n",
      "finished 88000/183949\n",
      "finished 89000/183949\n",
      "finished 90000/183949\n",
      "finished 91000/183949\n",
      "finished 92000/183949\n",
      "finished 93000/183949\n",
      "finished 94000/183949\n",
      "finished 95000/183949\n",
      "finished 96000/183949\n",
      "finished 97000/183949\n",
      "finished 98000/183949\n",
      "finished 99000/183949\n",
      "finished 100000/183949\n",
      "finished 101000/183949\n",
      "finished 102000/183949\n",
      "finished 103000/183949\n",
      "finished 104000/183949\n",
      "finished 105000/183949\n",
      "finished 106000/183949\n",
      "finished 107000/183949\n",
      "finished 108000/183949\n",
      "finished 109000/183949\n",
      "finished 110000/183949\n",
      "finished 111000/183949\n",
      "finished 112000/183949\n",
      "finished 113000/183949\n",
      "finished 114000/183949\n",
      "finished 115000/183949\n",
      "finished 116000/183949\n",
      "finished 117000/183949\n",
      "finished 118000/183949\n",
      "finished 119000/183949\n",
      "finished 120000/183949\n",
      "finished 121000/183949\n",
      "finished 122000/183949\n",
      "finished 123000/183949\n",
      "finished 124000/183949\n",
      "finished 125000/183949\n",
      "finished 126000/183949\n",
      "finished 127000/183949\n",
      "finished 128000/183949\n",
      "finished 129000/183949\n",
      "finished 130000/183949\n",
      "finished 131000/183949\n",
      "finished 132000/183949\n",
      "finished 133000/183949\n",
      "finished 134000/183949\n",
      "finished 135000/183949\n",
      "finished 136000/183949\n",
      "finished 137000/183949\n",
      "finished 138000/183949\n",
      "finished 139000/183949\n",
      "finished 140000/183949\n",
      "finished 141000/183949\n",
      "finished 142000/183949\n",
      "finished 143000/183949\n",
      "finished 144000/183949\n",
      "finished 145000/183949\n",
      "finished 146000/183949\n",
      "finished 147000/183949\n",
      "finished 148000/183949\n",
      "finished 149000/183949\n",
      "finished 150000/183949\n",
      "finished 151000/183949\n",
      "finished 152000/183949\n",
      "finished 153000/183949\n",
      "finished 154000/183949\n",
      "finished 155000/183949\n",
      "finished 156000/183949\n",
      "finished 157000/183949\n",
      "finished 158000/183949\n",
      "finished 159000/183949\n",
      "finished 160000/183949\n",
      "finished 161000/183949\n",
      "finished 162000/183949\n",
      "finished 163000/183949\n",
      "finished 164000/183949\n",
      "finished 165000/183949\n",
      "finished 166000/183949\n",
      "finished 167000/183949\n",
      "finished 168000/183949\n",
      "finished 169000/183949\n",
      "finished 170000/183949\n",
      "finished 171000/183949\n",
      "finished 172000/183949\n",
      "finished 173000/183949\n",
      "finished 174000/183949\n",
      "finished 175000/183949\n",
      "finished 176000/183949\n",
      "finished 177000/183949\n",
      "finished 178000/183949\n",
      "finished 179000/183949\n",
      "finished 180000/183949\n",
      "finished 181000/183949\n",
      "finished 182000/183949\n",
      "finished 183000/183949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>topic7</td>\n",
       "      <td>chatgpt,ai,use,work,like,job,creat,googl,gener...</td>\n",
       "      <td>ai take job within five year expert warn exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>topic17</td>\n",
       "      <td>chatgpt,ai,work,use,job,write,educ,like,gener,...</td>\n",
       "      <td>although chatgpt rapidli gain popular also bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>topic3</td>\n",
       "      <td>chatgpt,ai,work,educ,use,job,write,think,intel...</td>\n",
       "      <td>amplifi human potenti school educ board readi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topic13</td>\n",
       "      <td>chatgpt,ai,use,job,work,like,make,tool,task,he...</td>\n",
       "      <td>analyst eran shimoni omer tsarfati detail crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>topic7</td>\n",
       "      <td>chatgpt,ai,use,work,like,job,creat,googl,gener...</td>\n",
       "      <td>artificialintellig ai take job within five yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183944</th>\n",
       "      <td>topic2</td>\n",
       "      <td>chatgpt,ai,work,use,like,job,help,task,educ,ma...</td>\n",
       "      <td>zzzz build high valu brand reason presenc tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183945</th>\n",
       "      <td>topic0</td>\n",
       "      <td>chatgpt,work,ai,use,job,write,openai,need,ask,...</td>\n",
       "      <td>zzzz nice work curiou plan adjust chatgpt star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183946</th>\n",
       "      <td>topic10</td>\n",
       "      <td>chatgpt,ai,work,job,use,like,openai,gener,way,...</td>\n",
       "      <td>zzzz year ai autom busi save compani hr week b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183947</th>\n",
       "      <td>topic6</td>\n",
       "      <td>chatgpt,ai,job,work,use,new,like,task,learn,as...</td>\n",
       "      <td>zzzzzz content book buy find digit copi guaran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183948</th>\n",
       "      <td>topic6</td>\n",
       "      <td>chatgpt,ai,job,work,use,new,like,task,learn,as...</td>\n",
       "      <td>zzzzzz offici babi gpt project join ride moon ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic                                           keywords   \n",
       "0        topic7  chatgpt,ai,use,work,like,job,creat,googl,gener...  \\\n",
       "1       topic17  chatgpt,ai,work,use,job,write,educ,like,gener,...   \n",
       "2        topic3  chatgpt,ai,work,educ,use,job,write,think,intel...   \n",
       "3       topic13  chatgpt,ai,use,job,work,like,make,tool,task,he...   \n",
       "4        topic7  chatgpt,ai,use,work,like,job,creat,googl,gener...   \n",
       "...         ...                                                ...   \n",
       "183944   topic2  chatgpt,ai,work,use,like,job,help,task,educ,ma...   \n",
       "183945   topic0  chatgpt,work,ai,use,job,write,openai,need,ask,...   \n",
       "183946  topic10  chatgpt,ai,work,job,use,like,openai,gener,way,...   \n",
       "183947   topic6  chatgpt,ai,job,work,use,new,like,task,learn,as...   \n",
       "183948   topic6  chatgpt,ai,job,work,use,new,like,task,learn,as...   \n",
       "\n",
       "                                                    tweet  \n",
       "0       ai take job within five year expert warn exper...  \n",
       "1       although chatgpt rapidli gain popular also bec...  \n",
       "2       amplifi human potenti school educ board readi ...  \n",
       "3       analyst eran shimoni omer tsarfati detail crea...  \n",
       "4       artificialintellig ai take job within five yea...  \n",
       "...                                                   ...  \n",
       "183944  zzzz build high valu brand reason presenc tool...  \n",
       "183945  zzzz nice work curiou plan adjust chatgpt star...  \n",
       "183946  zzzz year ai autom busi save compani hr week b...  \n",
       "183947  zzzzzz content book buy find digit copi guaran...  \n",
       "183948  zzzzzz offici babi gpt project join ride moon ...  \n",
       "\n",
       "[183949 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "topic_result_path = \"./output/topic_result_2023-04-22-13-40-46_1iter.txt\"\n",
    "reader = open(topic_result_path, \"r\")\n",
    "i = 0\n",
    "for line in reader.readlines():\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"finished {i}/{183949}\")\n",
    "    row = []\n",
    "\n",
    "    split_line = line.split(\"(\")\n",
    "    doc = split_line[0]\n",
    "    topic_id = int(split_line[1].split(\" \")[1].split(')')[0])\n",
    "    row.append(f\"topic{topic_id}\")\n",
    "    row.append(topic_all_words[topic_id])\n",
    "    row.append(doc)\n",
    "    temp_df = pd.DataFrame([row], columns=['topic', 'keywords', 'tweet'])\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    i += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_path='./output/tweets_by_topic.csv'\n",
    "\n",
    "df.to_csv(save_table_path, sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('btm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef337ba82591e8ad80352ba8fbdba14af60eec38c8eaf90bf9232c7a9f4fb1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
